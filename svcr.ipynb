{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SequentialSampler, RandomSampler, BatchSampler\n",
    "from torchvision import models, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.clean_labels import clean_labels\n",
    "from utils.prepare_images import prepare_images\n",
    "from utils.build_dataset import SVRCDataset\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put videos here!\n",
    "video_base = 'data/videos'\n",
    "videos = os.listdir(video_base)\n",
    "# images will be output to here\n",
    "image_base = 'data/images'\n",
    "if not os.path.exists(image_base):\n",
    "    os.mkdir(image_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line: \n",
    "# ffmpeg -i {input_video} -r {frame_rate} [-f {force_format} (not needed)] {output_images}\n",
    "# doc: https://ffmpeg.org/ffmpeg.html\n",
    "for video in videos:\n",
    "    input_path = os.path.join(video_base, video)\n",
    "    # make dirs\n",
    "    output_base = image_base + '/{}'.format(video.split('.')[0])\n",
    "    if not os.path.exists(output_base):\n",
    "        os.mkdir(output_base)\n",
    "    output_path = os.path.join(output_base, '%d.png')\n",
    "    # # command\n",
    "    # print('Frames extracted from {} to {}'.format(input_path, output_path))\n",
    "    # !ffmpeg -i {input_path} -r 1 {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_path = 'data/labels/video.phase.trainingData.clean.StudentVersion.csv'\n",
    "names_path = 'data/labels/names.csv'\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "names_df = pd.DataFrame({'Name': list(set(labels_df['PhaseName'].to_list()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/harrypotter/Documents/courses/dl/assignment2/e6691-2022spring-assign2-vcsz/utils/prepare_images.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  video_df[['StartSec', 'EndSec']] = video_df[['Start', 'End']].applymap(time2int)\n",
      "/Users/harrypotter/Documents/courses/dl/assignment2/e6691-2022spring-assign2-vcsz/utils/prepare_images.py:33: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  video_df[['StartSec', 'EndSec']] = video_df[['Start', 'End']].applymap(time2int)\n"
     ]
    }
   ],
   "source": [
    "prepare_images(video_base, image_base, labels_df, names_df, 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for video in videos[:2]:\n",
    "    base = os.path.join(image_base, video.split('.')[0])\n",
    "    image_paths += list(map(\n",
    "        lambda img: os.path.join(base, img), \n",
    "        os.listdir(base)\n",
    "    ))\n",
    "    labels += list(map(\n",
    "        lambda img: int(img.split('.')[0].split('-')[1]), \n",
    "        os.listdir(base)\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_images(x):\n",
    "    vid = int(x[0].split('_')[-1].split('/')[0])\n",
    "    frame = int(x[0].split('/')[-1].split('-')[0])\n",
    "    return vid*7200 + frame\n",
    "\n",
    "image_paths_lstm = []\n",
    "labels_lstm = []\n",
    "for path,label in sorted(zip(image_paths, labels), key=sort_images):\n",
    "    image_paths_lstm.append(path)\n",
    "    labels_lstm.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters LSTM\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 30\n",
    "EPOCHS = 5\n",
    "TRAIN_SIZE = int(0.7 * len(image_paths))\n",
    "TEST_SIZE = len(image_paths) - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "num_labels = 14\n",
    "\n",
    "# define transforms\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVRC,self).__init__()\n",
    "        # ResNet-18\n",
    "        self.resnet18 = nn.Sequential(*(\n",
    "            list(\n",
    "                models.resnet18(pretrained=True).children()\n",
    "            )[:-1]\n",
    "        ))\n",
    "        #self.resnet18.eval()\n",
    "        self.pretrain = True\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(512,512)\n",
    "        self.lstm_states = None\n",
    "        # FC\n",
    "        self.full = nn.Linear(512,num_labels)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.resnet18(x)\n",
    "        # Reshape\n",
    "        #print(x.shape)\n",
    "        if not self.pretrain:\n",
    "            x = x.view(3,10,-1) # time step, batch size\n",
    "            x,s = self.lstm(x, self.lstm_states)\n",
    "            # save lstm states\n",
    "            self.lstm_states = (s[0].detach(), s[1].detach())\n",
    "        x = self.full(x.view(-1,512))\n",
    "        return x #if self.pretrain else nn.Softmax(1)(x).view(30,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRCDataset(Dataset):\n",
    "    def __init__(self, image_path: list, image_class: list, transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.image_class = image_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, item): #can add more rules to pick data\n",
    "        img = Image.open(self.image_path[item])\n",
    "        label = self.image_class[item]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {'feature': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetTrainVal(object):\n",
    "    def __init__(self, model) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, labels, features, transform):\n",
    "        print('Training ResNet: ')\n",
    "\n",
    "        dataset = SVRCDataset(features, labels, transform)\n",
    "        train, test = random_split(dataset, [TRAIN_SIZE, TEST_SIZE])\n",
    "        train_loader = DataLoader(train, BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        self.model.pretrain = True\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "                features  = data['feature'].float()\n",
    "                labels = data['label']\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                train_acc += (preds==labels).sum().item()\n",
    "\n",
    "            train_loss /= len(train)\n",
    "            train_acc /= len(train)\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            valid_acc = 0.0\n",
    "            total = 0\n",
    "            self.model.eval()\n",
    "            for i, data in enumerate(test_loader):\n",
    "                features = data['feature']\n",
    "                labels = data['label']\n",
    "\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions,labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                valid_acc += (preds==labels).sum().item()\n",
    "                total += features.size(0)\n",
    "\n",
    "            valid_loss /= len(test)\n",
    "            valid_acc /= len(test)\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch+1} Training Loss: {train_loss} Train_acc: {train_acc}'\n",
    "                f'|| Validation Loss: {valid_loss} Valid_acc: {valid_acc}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTrainVal(object):\n",
    "    def __init__(self, model) -> None:\n",
    "        self.model = model\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, labels, features, transform, eval_intval=5):\n",
    "        dataset = SVRCDataset(features, labels, transform)\n",
    "        data_loader = DataLoader(\n",
    "            dataset, batch_sampler=BatchSampler(\n",
    "                SequentialSampler(dataset), \n",
    "                BATCH_SIZE, \n",
    "                drop_last=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model.pretrain = False\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            if (epoch + 1) % eval_intval == 0:\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model.lstm.train()\n",
    "                self.model.full.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "\n",
    "            for i, data in enumerate(data_loader):\n",
    "                features  = data['feature'].float()\n",
    "                labels = data['label']\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "\n",
    "                if not (epoch + 1) % eval_intval == 0:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                train_acc += (preds==labels).sum().item()\n",
    "\n",
    "            train_loss /= len(dataset)\n",
    "            train_acc /= len(dataset)\n",
    "\n",
    "            print('Epoch {} - {} Loss: {} Acc: {}'.format(\n",
    "                epoch+1, 'Train' if not (epoch + 1) % eval_intval == 0 else 'Valid', \n",
    "                train_loss, train_acc\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "WeightsPath = './models/weights_resnet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet: \n",
      "Epoch 1 Training Loss: 0.0318411965204706 Train_acc: 0.721656976744186|| Validation Loss: 0.012854344058662098 Valid_acc: 0.8950930626057529\n",
      "Epoch 2 Training Loss: 0.014724150559929915 Train_acc: 0.8851744186046512|| Validation Loss: 0.009833391963830453 Valid_acc: 0.9069373942470389\n",
      "Epoch 3 Training Loss: 0.01441638125648159 Train_acc: 0.8728197674418605|| Validation Loss: 0.02480492308458499 Valid_acc: 0.7681895093062606\n",
      "Epoch 4 Training Loss: 0.012177292273791378 Train_acc: 0.8917151162790697|| Validation Loss: 0.00720266179006717 Valid_acc: 0.9475465313028765\n",
      "Epoch 5 Training Loss: 0.00837168369853739 Train_acc: 0.9244186046511628|| Validation Loss: 0.03109713118088427 Valid_acc: 0.7749576988155669\n",
      "Time:2.9min\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = image_paths\n",
    "y = labels\n",
    "\n",
    "# resnet18 Model\n",
    "model = SVRC()\n",
    "#print(model)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.pretrain = True\n",
    "trainer = ResnetTrainVal(model)\n",
    "trainer.train(y, X, data_transform['train'])\n",
    "torch.save(model.state_dict(),WeightsPath+'1')\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time:{:.2}min'.format((end_time-start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.09230332366694154 Acc: 0.3035078800203355\n",
      "Epoch 2 - Train Loss: 0.09115437841318438 Acc: 0.17386883579054396\n",
      "Epoch 3 - Train Loss: 0.082175728386286 Acc: 0.13726487036095578\n",
      "Epoch 4 - Train Loss: 0.07780717367076244 Acc: 0.13726487036095578\n",
      "Epoch 5 - Valid Loss: 0.06773507595062256 Acc: 0.2846975088967972\n",
      "Time:3.0min\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = image_paths_lstm\n",
    "y = labels_lstm\n",
    "\n",
    "# SVRC Model\n",
    "model = SVRC()\n",
    "#print(model)\n",
    "\n",
    "model.pretrain = False\n",
    "model.load_state_dict(torch.load(WeightsPath+'1'))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer = LstmTrainVal(model)\n",
    "trainer.train(y, X, data_transform['train'])\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time:{:.2}min'.format((end_time-start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bb744823c5315bc838d6f85bb474c2716845bb0b4d758ac389cca5a4bd648da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
