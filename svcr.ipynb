{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11.3\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, SequentialSampler, RandomSampler, BatchSampler\n",
    "from torchvision import models, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from utils.clean_labels import clean_labels\n",
    "from utils.prepare_images import prepare_images\n",
    "from utils.build_dataset import SVRCDataset\n",
    "\n",
    "import os\n",
    "from datetime import datetime\n",
    "import random\n",
    "import time\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "print(torch.version.cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put videos here!\n",
    "# video_base = 'data/videos'\n",
    "video_base = 'D:/e6691/6691_assignment2/videos'\n",
    "videos = os.listdir(video_base)\n",
    "# images will be output to here\n",
    "# image_base = 'data/images'\n",
    "image_base = 'D:/e6691/6691_assignment2/images'\n",
    "if not os.path.exists(image_base):\n",
    "    os.mkdir(image_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# command line: \n",
    "# ffmpeg -i {input_video} -r {frame_rate} [-f {force_format} (not needed)] {output_images}\n",
    "# doc: https://ffmpeg.org/ffmpeg.html\n",
    "for video in videos:\n",
    "    input_path = os.path.join(video_base, video)\n",
    "    # make dirs\n",
    "    output_base = image_base + '/{}'.format(video.split('.')[0])\n",
    "    if not os.path.exists(output_base):\n",
    "        os.mkdir(output_base)\n",
    "    output_path = os.path.join(output_base, '%d.png')\n",
    "    # # command\n",
    "    # print('Frames extracted from {} to {}'.format(input_path, output_path))\n",
    "    # !ffmpeg -i {input_path} -r 1 {output_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels_path = 'data/labels/video.phase.trainingData.clean.StudentVersion.csv'\n",
    "labels_path = 'D:/e6691/6691_assignment2/labels/video.phase.trainingData.clean.StudentVersion.csv'\n",
    "# names_path = 'data/labels/names.csv'\n",
    "names_path = 'D:/e6691/6691_assignment2/labels/names.csv'\n",
    "labels_df = pd.read_csv(labels_path)\n",
    "names_df = pd.DataFrame({'Name': list(set(labels_df['PhaseName'].to_list()))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\python\\lib\\site-packages\\pandas\\core\\frame.py:3641: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self[k1] = value[k2]\n"
     ]
    }
   ],
   "source": [
    "prepare_images(video_base, image_base, labels_df, names_df, 'png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all images and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for video in videos[:]:\n",
    "    base = os.path.join(image_base, video.split('.')[0])\n",
    "    # image_paths += list(map(\n",
    "    #     lambda img: os.path.join(base, img), \n",
    "    #     os.listdir(base)\n",
    "    # ))\n",
    "    image_paths += list(map(\n",
    "        lambda img: base + '/' + img,\n",
    "        os.listdir(base)\n",
    "    ))\n",
    "    labels += list(map(\n",
    "        lambda img: int(img.split('.')[0].split('-')[1]), \n",
    "        os.listdir(base)\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get 2 images and labels\n",
    "image_paths = []\n",
    "labels = []\n",
    "\n",
    "for video in videos[:2]:\n",
    "    base = os.path.join(image_base, video.split('.')[0])\n",
    "    # image_paths += list(map(\n",
    "    #     lambda img: os.path.join(base, img), \n",
    "    #     os.listdir(base)\n",
    "    # ))\n",
    "    image_paths += list(map(\n",
    "        lambda img: base + '/' + img,\n",
    "        os.listdir(base)\n",
    "    ))\n",
    "    labels += list(map(\n",
    "        lambda img: int(img.split('.')[0].split('-')[1]), \n",
    "        os.listdir(base)\n",
    "    ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_images(x):\n",
    "    vid = int(x[0].split('_')[-1].split('/')[0])\n",
    "    frame = int(x[0].split('/')[-1].split('-')[0])\n",
    "    return vid*7200 + frame\n",
    "\n",
    "image_paths_lstm = []\n",
    "labels_lstm = []\n",
    "for path,label in sorted(zip(image_paths, labels), key=sort_images):\n",
    "    image_paths_lstm.append(path)\n",
    "    labels_lstm.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters LSTM\n",
    "LR = 1e-3\n",
    "BATCH_SIZE = 3\n",
    "EPOCHS = 5\n",
    "TRAIN_SIZE = int(0.7 * len(image_paths))\n",
    "TEST_SIZE = len(image_paths) - TRAIN_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of labels\n",
    "num_labels = 14\n",
    "\n",
    "# define transforms\n",
    "data_transform = {\n",
    "    \"train\": transforms.Compose([\n",
    "        transforms.Resize((32,32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485], std=[0.229])\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SVRC,self).__init__()\n",
    "        # ResNet-18\n",
    "        self.resnet18 = nn.Sequential(*(\n",
    "            list(\n",
    "                models.resnet18(pretrained=True).children()\n",
    "            )[:-1]\n",
    "        ))\n",
    "        #self.resnet18.eval()\n",
    "        self.pretrain = True\n",
    "        # LSTM\n",
    "        self.lstm = nn.LSTM(512,512)\n",
    "        self.lstm_states = None\n",
    "        # FC\n",
    "        self.full = nn.Linear(512,num_labels)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = self.resnet18(x)\n",
    "        # Reshape\n",
    "        #print(x.shape)\n",
    "        if not self.pretrain:\n",
    "            x = x.view(3,1,-1) # time step, batch size\n",
    "            x,s = self.lstm(x, self.lstm_states)\n",
    "            # save lstm states\n",
    "            self.lstm_states = (s[0].detach(), s[1].detach())\n",
    "            \n",
    "        x = self.full(x.view(-1,512))\n",
    "        return x #if self.pretrain else nn.Softmax(1)(x).view(30,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVRCDataset(Dataset):\n",
    "    def __init__(self, image_path: list, image_class: list, transform=None):\n",
    "        self.image_path = image_path\n",
    "        self.image_class = image_class\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_path)\n",
    "\n",
    "    def __getitem__(self, item): #can add more rules to pick data\n",
    "        img = Image.open(self.image_path[item])\n",
    "        label = self.image_class[item]\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return {'feature': img, 'label': label}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResnetTrainVal(object):\n",
    "    def __init__(self, model, device) -> None:\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, labels, features, transform):\n",
    "        print('Training ResNet: ')\n",
    "\n",
    "        dataset = SVRCDataset(features, labels, transform)\n",
    "        train, test = random_split(dataset, [TRAIN_SIZE, TEST_SIZE])\n",
    "        print(len(train))\n",
    "        train_loader = DataLoader(train, BATCH_SIZE, shuffle=True)\n",
    "        test_loader = DataLoader(test, BATCH_SIZE, shuffle=True)\n",
    "\n",
    "        self.model.pretrain = True\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            self.model.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "\n",
    "            for i, data in enumerate(train_loader):\n",
    "            \n",
    "                features = data['feature'].float()\n",
    "                labels = data['label']\n",
    "            \n",
    "                # features  = data['feature'].float()\n",
    "                # labels = data['label']\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                self.optimizer.zero_grad()\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                train_acc += (preds==labels).sum().item()\n",
    "            \n",
    "            train_loss /= len(train)\n",
    "            train_acc /= len(train)\n",
    "\n",
    "            valid_loss = 0.0\n",
    "            valid_acc = 0.0\n",
    "            total = 0\n",
    "            self.model.eval()\n",
    "            for i, data in enumerate(test_loader):\n",
    "                features = data['feature']\n",
    "                labels = data['label']\n",
    "\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions,labels)\n",
    "                valid_loss += loss.item()\n",
    "\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                valid_acc += (preds==labels).sum().item()\n",
    "                total += features.size(0)\n",
    "\n",
    "            valid_loss /= len(test)\n",
    "            valid_acc /= len(test)\n",
    "\n",
    "            print(\n",
    "                f'Epoch {epoch+1} Training Loss: {train_loss} Train_acc: {train_acc}'\n",
    "                f'|| Validation Loss: {valid_loss} Valid_acc: {valid_acc}'\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LstmTrainVal(object):\n",
    "    def __init__(self, model,device) -> None:\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=LR)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    def train(self, labels, features, transform, eval_intval=5):\n",
    "        dataset = SVRCDataset(features, labels, transform)\n",
    "        data_loader = DataLoader(\n",
    "            dataset, batch_sampler=BatchSampler(\n",
    "                SequentialSampler(dataset), \n",
    "                BATCH_SIZE, \n",
    "                drop_last=True\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.model.pretrain = False\n",
    "\n",
    "        for epoch in range(EPOCHS):\n",
    "            if (epoch + 1) % eval_intval == 0:\n",
    "                self.model.eval()\n",
    "            else:\n",
    "                self.model.lstm.train()\n",
    "                self.model.full.train()\n",
    "\n",
    "            train_loss = 0.0\n",
    "            train_acc = 0.0\n",
    "\n",
    "            for i, data in enumerate(data_loader):\n",
    "                features  = data['feature'].float()\n",
    "                \n",
    "                labels = data['label']\n",
    "                features, labels = features.to(self.device), labels.to(self.device)\n",
    "                predictions = self.model(features)\n",
    "                loss = self.criterion(predictions, labels)\n",
    "\n",
    "                if not (epoch + 1) % eval_intval == 0:\n",
    "                    self.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    self.optimizer.step()\n",
    "\n",
    "                train_loss += loss.item()\n",
    "                preds = torch.max(predictions.data, 1)[1]\n",
    "                train_acc += (preds==labels).sum().item()\n",
    "\n",
    "            train_loss /= len(dataset)\n",
    "            train_acc /= len(dataset)\n",
    "\n",
    "            print('Epoch {} - {} Loss: {} Acc: {} LSTM'.format(\n",
    "                epoch+1, 'Train' if not (epoch + 1) % eval_intval == 0 else 'Valid', \n",
    "                train_loss, train_acc\n",
    "            ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Weights\n",
    "WeightsPath = './models/weights_resnet18'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet: \n",
      "3615\n",
      "Epoch 1 Training Loss: 0.8077436924640239 Train_acc: 0.244536652835408|| Validation Loss: 0.7426833113547294 Valid_acc: 0.26064516129032256\n",
      "Epoch 2 Training Loss: 0.7773627837663865 Train_acc: 0.22461964038727525|| Validation Loss: 0.7359198786750917 Valid_acc: 0.2496774193548387\n",
      "Epoch 3 Training Loss: 0.760970380873436 Train_acc: 0.2359612724757953|| Validation Loss: 1.3449061864422214 Valid_acc: 0.24129032258064517\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32md:\\桌面\\e6691-2022spring-assign2-vcsz\\svcr.ipynb Cell 17'\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000016?line=12'>13</a>\u001b[0m model\u001b[39m.\u001b[39mpretrain \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000016?line=13'>14</a>\u001b[0m trainer \u001b[39m=\u001b[39m ResnetTrainVal(model, device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000016?line=14'>15</a>\u001b[0m trainer\u001b[39m.\u001b[39;49mtrain(y, X, data_transform[\u001b[39m'\u001b[39;49m\u001b[39mtrain\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000016?line=16'>17</a>\u001b[0m torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(),WeightsPath\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m1\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000016?line=18'>19</a>\u001b[0m end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n",
      "\u001b[1;32md:\\桌面\\e6691-2022spring-assign2-vcsz\\svcr.ipynb Cell 14'\u001b[0m in \u001b[0;36mResnetTrainVal.train\u001b[1;34m(self, labels, features, transform)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000013?line=21'>22</a>\u001b[0m train_loss \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000013?line=22'>23</a>\u001b[0m train_acc \u001b[39m=\u001b[39m \u001b[39m0.0\u001b[39m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000013?line=24'>25</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_loader):\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000013?line=26'>27</a>\u001b[0m     features \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mfeature\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000013?line=27'>28</a>\u001b[0m     labels \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:521\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=518'>519</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=519'>520</a>\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=520'>521</a>\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=521'>522</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=522'>523</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=523'>524</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=524'>525</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:561\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=558'>559</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=559'>560</a>\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=560'>561</a>\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=561'>562</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataloader.py?line=562'>563</a>\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data)\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=46'>47</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=47'>48</a>\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[1;32m---> <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=48'>49</a>\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=49'>50</a>\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/_utils/fetch.py?line=50'>51</a>\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\utils\\data\\dataset.py:363\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataset.py?line=360'>361</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(idx, \u001b[39mlist\u001b[39m):\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataset.py?line=361'>362</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mindices[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m idx]]\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torch/utils/data/dataset.py?line=362'>363</a>\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mindices[idx]]\n",
      "\u001b[1;32md:\\桌面\\e6691-2022spring-assign2-vcsz\\svcr.ipynb Cell 13'\u001b[0m in \u001b[0;36mSVRCDataset.__getitem__\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000012?line=11'>12</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_class[item]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000012?line=12'>13</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000012?line=13'>14</a>\u001b[0m     img \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(img)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/%E6%A1%8C%E9%9D%A2/e6691-2022spring-assign2-vcsz/svcr.ipynb#ch0000012?line=15'>16</a>\u001b[0m \u001b[39mreturn\u001b[39;00m {\u001b[39m'\u001b[39m\u001b[39mfeature\u001b[39m\u001b[39m'\u001b[39m: img, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m: label}\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms\\transforms.py:61\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=58'>59</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=59'>60</a>\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[1;32m---> <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=60'>61</a>\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[0;32m     <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=61'>62</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torch\\nn\\modules\\module.py:1102\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1097'>1098</a>\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1098'>1099</a>\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1099'>1100</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1100'>1101</a>\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1101'>1102</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1102'>1103</a>\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/torch/nn/modules/module.py?line=1103'>1104</a>\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms\\transforms.py:304\u001b[0m, in \u001b[0;36mResize.forward\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=295'>296</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=296'>297</a>\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=297'>298</a>\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=298'>299</a>\u001b[0m \u001b[39m        img (PIL Image or Tensor): Image to be scaled.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=301'>302</a>\u001b[0m \u001b[39m        PIL Image or Tensor: Rescaled image.\u001b[39;00m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=302'>303</a>\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/transforms.py?line=303'>304</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mresize(img, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msize, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minterpolation, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_size, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mantialias)\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms\\functional.py:419\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size, antialias)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=414'>415</a>\u001b[0m         warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=415'>416</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mAnti-alias option is always applied for PIL Image input. Argument antialias is ignored.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=416'>417</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=417'>418</a>\u001b[0m     pil_interpolation \u001b[39m=\u001b[39m pil_modes_mapping[interpolation]\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=418'>419</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m F_pil\u001b[39m.\u001b[39;49mresize(img, size\u001b[39m=\u001b[39;49msize, interpolation\u001b[39m=\u001b[39;49mpil_interpolation, max_size\u001b[39m=\u001b[39;49mmax_size)\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional.py?line=420'>421</a>\u001b[0m \u001b[39mreturn\u001b[39;00m F_t\u001b[39m.\u001b[39mresize(img, size\u001b[39m=\u001b[39msize, interpolation\u001b[39m=\u001b[39minterpolation\u001b[39m.\u001b[39mvalue, max_size\u001b[39m=\u001b[39mmax_size, antialias\u001b[39m=\u001b[39mantialias)\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\torchvision\\transforms\\functional_pil.py:265\u001b[0m, in \u001b[0;36mresize\u001b[1;34m(img, size, interpolation, max_size)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=259'>260</a>\u001b[0m \u001b[39mif\u001b[39;00m max_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=260'>261</a>\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=261'>262</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mmax_size should only be passed if size specifies the length of the smaller edge, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=262'>263</a>\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=263'>264</a>\u001b[0m     )\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/torchvision/transforms/functional_pil.py?line=264'>265</a>\u001b[0m \u001b[39mreturn\u001b[39;00m img\u001b[39m.\u001b[39;49mresize(size[::\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m], interpolation)\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\PIL\\Image.py:1982\u001b[0m, in \u001b[0;36mImage.resize\u001b[1;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/PIL/Image.py?line=1978'>1979</a>\u001b[0m     im \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39mresize(size, resample, box)\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/PIL/Image.py?line=1979'>1980</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m im\u001b[39m.\u001b[39mconvert(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode)\n\u001b[1;32m-> <a href='file:///d%3A/python/lib/site-packages/PIL/Image.py?line=1981'>1982</a>\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload()\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/PIL/Image.py?line=1983'>1984</a>\u001b[0m \u001b[39mif\u001b[39;00m reducing_gap \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m resample \u001b[39m!=\u001b[39m NEAREST:\n\u001b[0;32m   <a href='file:///d%3A/python/lib/site-packages/PIL/Image.py?line=1984'>1985</a>\u001b[0m     factor_x \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m((box[\u001b[39m2\u001b[39m] \u001b[39m-\u001b[39m box[\u001b[39m0\u001b[39m]) \u001b[39m/\u001b[39m size[\u001b[39m0\u001b[39m] \u001b[39m/\u001b[39m reducing_gap) \u001b[39mor\u001b[39;00m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32md:\\python\\lib\\site-packages\\PIL\\ImageFile.py:255\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=248'>249</a>\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=249'>250</a>\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mimage file is truncated \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=250'>251</a>\u001b[0m             \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m(\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mlen\u001b[39m(b)\u001b[39m}\u001b[39;00m\u001b[39m bytes not processed)\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=251'>252</a>\u001b[0m         )\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=253'>254</a>\u001b[0m b \u001b[39m=\u001b[39m b \u001b[39m+\u001b[39m s\n\u001b[1;32m--> <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=254'>255</a>\u001b[0m n, err_code \u001b[39m=\u001b[39m decoder\u001b[39m.\u001b[39;49mdecode(b)\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=255'>256</a>\u001b[0m \u001b[39mif\u001b[39;00m n \u001b[39m<\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    <a href='file:///d%3A/python/lib/site-packages/PIL/ImageFile.py?line=256'>257</a>\u001b[0m     \u001b[39mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = image_paths\n",
    "y = labels\n",
    "\n",
    "# resnet18 Model\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "model = SVRC()\n",
    "if torch.cuda.is_available:\n",
    "    model.to(device)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "model.pretrain = True\n",
    "trainer = ResnetTrainVal(model, device)\n",
    "trainer.train(y, X, data_transform['train'])\n",
    "\n",
    "torch.save(model.state_dict(),WeightsPath+'1')\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time:{:.2}min'.format((end_time-start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Train Loss: 0.09230332366694154 Acc: 0.3035078800203355\n",
      "Epoch 2 - Train Loss: 0.09115437841318438 Acc: 0.17386883579054396\n",
      "Epoch 3 - Train Loss: 0.082175728386286 Acc: 0.13726487036095578\n",
      "Epoch 4 - Train Loss: 0.07780717367076244 Acc: 0.13726487036095578\n",
      "Epoch 5 - Valid Loss: 0.06773507595062256 Acc: 0.2846975088967972\n",
      "Time:3.0min\n"
     ]
    }
   ],
   "source": [
    "# Data\n",
    "X = image_paths_lstm\n",
    "y = labels_lstm\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available else 'cpu')\n",
    "# SVRC Model\n",
    "model = SVRC()\n",
    "if torch.cuda.is_available:\n",
    "    model.to(device)\n",
    "#print(model)\n",
    "\n",
    "model.pretrain = False\n",
    "model.load_state_dict(torch.load(WeightsPath+'1'))\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "trainer = LstmTrainVal(model, device)\n",
    "trainer.train(y, X, data_transform['train'])\n",
    "\n",
    "end_time = time.time()\n",
    "print('Time:{:.2}min'.format((end_time-start_time)/60.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7bb744823c5315bc838d6f85bb474c2716845bb0b4d758ac389cca5a4bd648da"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('dl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
